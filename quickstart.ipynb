{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HuggingFace の `datasets` を用いてMNIST",
   "id": "f81c2ec5764f02a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. 必要なライブラリ",
   "id": "f38d9f452444b053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
   "id": "9f54ba77dc99e36a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install datasets transformers accelerate evaluate",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install scikit-learn",
   "id": "4b1c13a1a3a64273",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T00:53:07.692593Z",
     "start_time": "2025-07-20T00:53:06.618462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ],
   "id": "ed7dbec489c2727d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. データの読み込み",
   "id": "e4e13595ca7cc209"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T00:53:47.487885Z",
     "start_time": "2025-07-20T00:53:39.664506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# MNISTの読み込み（28×28モノクロ画像）\n",
    "dataset = load_dataset(\"mnist\")\n",
    "print(dataset)"
   ],
   "id": "9087a137b90db882",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 60000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 画像の変換処理（28×28 → 224×224）",
   "id": "2ecbf3a010a565e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T03:42:58.517218Z",
     "start_time": "2025-07-20T00:53:51.146512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Grayscale\n",
    "\n",
    "# 画像前処理器（モデルに合ったサイズにする）\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", use_fast=True)\n",
    "\n",
    "# torchvisionで画像変換\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    Grayscale(num_output_channels=3),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# 前処理関数を定義（datasets.map に使う）\n",
    "def preprocess(example):\n",
    "    image = transform(example[\"image\"])\n",
    "    example[\"pixel_values\"] = image\n",
    "    return example\n",
    "\n",
    "# 前処理を全データに適用\n",
    "dataset = dataset.map(preprocess)"
   ],
   "id": "863032610a653ae7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f745dc263ca47df98723443fc85c1ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d30d0b359e34d61b40d1c7f4a3b9e46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. ViTモデルの準備",
   "id": "cb808108b0f927c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T03:44:13.644760Z",
     "start_time": "2025-07-20T03:44:12.422541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=10  # MNISTは 0〜9\n",
    ")"
   ],
   "id": "14982004fd32c6ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. 学習と評価",
   "id": "3a8b785e4444fd1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:36:33.615815Z",
     "start_time": "2025-07-20T03:44:17.374390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mnist-vit\",\n",
    "    per_device_train_batch_size=32,\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,  # ViTでは ImageProcessor を tokenizer として使う\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "4c4307da132b0ab8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ArchWizard7\\AppData\\Local\\Temp\\ipykernel_15436\\1176347120.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 4:52:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.05180937139723036, metrics={'train_runtime': 17532.2719, 'train_samples_per_second': 10.267, 'train_steps_per_second': 0.321, 'total_flos': 1.394955826274304e+19, 'train_loss': 0.05180937139723036, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. 推論",
   "id": "2d15cb24ce1e35e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T12:03:05.909074Z",
     "start_time": "2025-07-20T12:03:04.763130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "image = dataset[\"test\"][0][\"image\"]\n",
    "inputs = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    pred = logits.argmax(-1).item()\n",
    "\n",
    "print(f\"予測: {pred}, 正解: {dataset['test'][0]['label']}\")"
   ],
   "id": "e39be8a04036b9f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測: 7, 正解: 7\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
